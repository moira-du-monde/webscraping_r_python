{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moira-du-monde/webscraping_r_python/blob/main/scraping_materials/Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PLEASE NOTE: This case study assumes you've already installed Python and Jupyter Notebook (see [these docs](https://docs.jupyter.org/en/latest/install/notebook-classic.html) to learn how to install on your own computer) and/or are able to run Jupyter Notebook on [Google Colab](https://www.geeksforgeeks.org/how-to-use-google-colab/)."
      ],
      "metadata": {
        "id": "LhVtaLwAMLz-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scraping the Cleveland Museum of Art's Online Collection\n",
        "A case study in webscraping, designed to complement the February 2023 workshop \"INTRO TO WEB SCRAPING USING R OR PYTHON\" presentation [slides](link). "
      ],
      "metadata": {
        "id": "g35Ov2vB10xi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## History and objective\n",
        "\n",
        "ðŸŽ¨ We are art historians interested in gathering data about ancient artworks of controversial or unknown provenance in the U.S.\n",
        "\n",
        "The statue [\"Apollo the Python Slayer\"](https://www.cleveland.com/arts/2013/09/the_cleveland_museum_of_art_wa.html) is one such artwork.  We know it is part of the\n",
        "[Cleveland Museum of Art's](https://www.clevelandart.org/) permanent collection, and we would like to add it to our master database.  \n",
        "\n",
        "Instead of manually copying this information from the website, which can take time and lead to errors, we will write a Python program that (1) scrapes *Apollo's* title, artist, geographical origin, medium, year, ownership history, and a description of how the museum came into posession of the piece, and (2) inserts the new record into our existing dataframe.\n"
      ],
      "metadata": {
        "id": "LIMDGUxYOrCC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process"
      ],
      "metadata": {
        "id": "B5WBWplm4qaN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.  Installing libraries**\n",
        "\n",
        "Begin by installing (if you haven't already) the two libraries we'll be using.\n",
        "\n",
        "The first, [Requests](https://requests.readthedocs.io/en/latest/), allows you to make HTTP/1.1 requests to a server (in effect, this establishes the connection) and to read in the target webpage's HTML code.\n",
        "\n",
        "Once we've made our requests, we can use [Beautiful Soup](https://beautiful-soup-4.readthedocs.io/en/latest/) to parse and extract data from this HTML script."
      ],
      "metadata": {
        "id": "WzikKlC-50pt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "NOTE: Our Statistical Consulting office heeds the first commandment of programming: \"thou shalt not use unnecessary packages and libraries.\" In that spirit, the packages we will use in this workshop have been carefully and sparingly selected."
      ],
      "metadata": {
        "id": "BdhNlR_IL69G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "## install requests"
      ],
      "metadata": {
        "id": "rgyViZpP2ATi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "## install beautifulsoup4"
      ],
      "metadata": {
        "id": "TyjNQ9EG2VQC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "After installing the libraries, go ahead and import them as well as [pandas](https://pandas.pydata.org/docs/), the pseudo-database management system we will use to organize and store our data.  Pandas is pre-installed on most Python IDEs."
      ],
      "metadata": {
        "id": "Qa7aayZiMcDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "pePdd90NLCx1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.  Making the GET Request**"
      ],
      "metadata": {
        "id": "HGJ_ColJ3MIr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are targeting a single webpage with a single URL, you'll just need a few lines of code to get started.  We already know what we are looking for so we will go right to the source."
      ],
      "metadata": {
        "id": "dLFj5HoRR0Gb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "apollo_url = 'https://www.clevelandart.org/art/2004.30'"
      ],
      "metadata": {
        "id": "FeHgxSNTghG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you've identified a target page, use the requests package to access its html."
      ],
      "metadata": {
        "id": "fCB3EVGM3qAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## get request\n",
        "\n",
        "## src usually the object label for the server's response\n"
      ],
      "metadata": {
        "id": "kGkCSGNK8pK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Parsing the HTML/XML**"
      ],
      "metadata": {
        "id": "nYds8z3ID0A3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we are ready to parse the text, we'll use the BeautifulSoup function to create an HTML tree."
      ],
      "metadata": {
        "id": "IL6TOEv-EDS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Parse the response into an HTML tree\n",
        "\n",
        "## lmxl is Python's HTML parser"
      ],
      "metadata": {
        "id": "iwHR4fgtRztb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to check your work and/or have the html handy in this window, run the next cell to print."
      ],
      "metadata": {
        "id": "IHcKqX0gE_hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Print to view the HTML using the \"prettify\" function which preserves the indentation\n",
        "\n",
        "\n",
        "## since we know our information is near the bottom of the page, we'll start printing at line 1000"
      ],
      "metadata": {
        "id": "ybyYWE1IEeZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Extracting the HTML elements of interest**\n",
        "\n",
        "As a reminder, we have set out to collect the artwork's title, date, artist, artist's country of origin, medium, previous owners, and a description of how the museum came into posession of the piece.  \n",
        "\n",
        "By examining the above HTML, we notice **title** information is held in the \"h1\" header tag and is of the class \"field field-name-field-primary-title field-type-text field-label-hidden\".\n",
        "\n",
        "We will pass those arguments to the \"soup\" function, which produces a list of all relevant parts of the HTML (it calls this list the \"Result Set\").  "
      ],
      "metadata": {
        "id": "iyPaPsUrKZvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "title = soup(\"h1\", \"field field-name-field-primary-title field-type-text field-label-hidden\") # soup function args : tag, class\n",
        "## print title and notice the output is a list\n",
        "\n",
        "print('Class: ', type(title)) "
      ],
      "metadata": {
        "id": "OrZhZxamNM3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To retrieve a single item from the list, select by the item's index to create an object of type \"bs4.element.Tag.\""
      ],
      "metadata": {
        "id": "wzk4qeX9abVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## select the first record and call the variable \"title_only\"\n",
        "print('Class: ', type(title_only)) # notice the data type for this record is a Beautiful Soup tag"
      ],
      "metadata": {
        "id": "UnZ2D7OEVySm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get the element's text only, call the **text** function on this Tag object."
      ],
      "metadata": {
        "id": "O4vFq9Chah8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "title_for_db = title_only.text\n",
        "print(title_for_db) # check this only outputs the relevant data"
      ],
      "metadata": {
        "id": "woW-BGLdXF8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have now collected our title data:\n",
        "\n",
        "1.   Title: \"h1\" header tag of class \"field field-name-field-primary-title field-type-text field-label-hidden\"\n",
        "\n",
        "Let's use the same workflow to get the rest of our data, which involves the following tags and class combinations:\n",
        "\n",
        "2.   Date: \"p\" paragraph of class \"field field-name-field-date-text field-type-text field-label-hidden\"\n",
        "\n",
        "3.   Artist: a combination of \"span\" container of class \"field field-name-field-artist-qualifier\" and \"span\" container of class \"field field-name-field-artist-name\"\n",
        "\n",
        "4.   Artist's country of origin: \"p\" paragraph of class \"\"field field-name-field-artist-origin\"\n",
        "\n",
        "5.   Medium: \"p\" paragraph of class \"field field-name-art-object-medium field-type-ds field-label-hidden\"\n",
        "\n",
        "6.   Ownership history: multiple (3) combinations of \"div\" container of class \"field field-name-field-provenance-description\" and \"field field-name-field-provenance-date\"\n",
        "\n",
        "7.   Museum possession: \"span\" of class \"field field-name-field-credit-line\"\n"
      ],
      "metadata": {
        "id": "_HICvPdPdrjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Date\n",
        "\n",
        "date = soup(\"p\", \"field field-name-field-date-text field-type-text field-label-hidden\")\n",
        "date_only = date[0]\n",
        "date_for_db = date_only.text\n",
        "\n",
        "## print(date_for_db)"
      ],
      "metadata": {
        "id": "axvtm3femgAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Artist\n",
        "\n",
        "qualifier = soup(\"span\", \"field field-name-field-artist-qualifier\")\n",
        "qual_for_db = qualifier[0].text\n",
        "\n",
        "# print(qual_for_db) # uncomment if you'd like to check your work\n",
        "\n",
        "artist = soup(\"span\", \"field field-name-field-artist-name\")\n",
        "artist_for_db = artist[0].text\n",
        "\n",
        "# print(artist_for_db) # uncomment if you'd like to check your work\n",
        "\n",
        "qual_artist_db = qual_for_db + \" \" + artist_for_db ## concatenate qualifier and artist with space in the middle\n",
        "\n",
        "##print out qual_artist_db to check work"
      ],
      "metadata": {
        "id": "Zzh8jjiGnMr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Artist origin\n",
        "\n",
        "origin = soup(\"p\", \"field field-name-field-artist-origin\")\n",
        "origin_for_db = origin[0].text\n",
        "\n",
        "print(origin_for_db)"
      ],
      "metadata": {
        "id": "6S71FoaIonRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Medium\n",
        "\n",
        "medium = soup(\"p\", \"field field-name-art-object-medium field-type-ds field-label-hidden\")\n",
        "medium_for_db = medium[0].text\n",
        "\n",
        "print(medium_for_db)"
      ],
      "metadata": {
        "id": "bJJVux-6qSRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ownership history\n",
        "\n",
        "ownership = soup(\"div\", \"field field-name-field-provenance-description\")\n",
        "owner_1 = ownership[0].text\n",
        "owner_2 = ownership[1].text\n",
        "owner_3 = ownership[2].text\n",
        "\n",
        "date_change = soup(\"div\",\"field field-name-field-provenance-date\")\n",
        "change_1 = date_change[0].text\n",
        "change_2 = date_change[1].text\n",
        "\n",
        "first = owner_1 + \" \" + change_1\n",
        "second = owner_2 + \" \" + change_2\n",
        "third = owner_3\n",
        "\n",
        "print(first)\n",
        "print(second)\n",
        "print(third)"
      ],
      "metadata": {
        "id": "mBoiu9igqFis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Funds used for purchase\n",
        "\n",
        "funds = soup(\"span\", \"field field-name-field-credit-line\")\n",
        "funds_for_db = funds[0].text\n",
        "\n",
        "print(funds_for_db)"
      ],
      "metadata": {
        "id": "agUL9Y6ku44a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple dataframe\n",
        "\n",
        "Run the code below to quickly reproduce the dataframe containing our other records and add the new data."
      ],
      "metadata": {
        "id": "8hdF-ClXaiaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dataframe\n",
        "data = [[0,0,0,0,0,0,0,0,0], [0,0,0,0,0,0,0,0,0]]\n",
        "df = pd.DataFrame(data, columns=['Title', 'Date', 'Artist', 'Artist_origin', 'Medium', 'Ownership_I','Ownership_II', 'Ownership_III','Funds_used_for_purchase'])"
      ],
      "metadata": {
        "id": "Qay_9yWqwBQE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add our scraped data\n",
        "new_data = [[title_for_db, date_for_db, qual_artist_db,origin_for_db,medium_for_db,first, second, third, funds_for_db]]\n",
        "new_data = pd.DataFrame(new_data, columns=['Title', 'Date', 'Artist', 'Artist_origin', 'Medium', 'Ownership_I','Ownership_II', 'Ownership_III','Funds_used_for_purchase'])\n",
        "df_whole = df.append(new_data, ignore_index=True)\n",
        "df_whole.head()"
      ],
      "metadata": {
        "id": "e1mzk1y4wHUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping using procedural programming\n",
        "\n",
        "This simple program does all of the above in just a few steps."
      ],
      "metadata": {
        "id": "ImTkrwx6RyPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# call the url\n",
        "apollo_url = 'https://www.clevelandart.org/art/2004.30'\n",
        "\n",
        "# get request\n",
        "req = requests.get(apollo_url)\n",
        "\n",
        "# src usually the object label for the server's response\n",
        "src = req.text\n",
        "\n",
        "# Parse the response into an HTML tree\n",
        "soup = BeautifulSoup(src, 'lxml') # lmxl is Python's HTML parser\n",
        "\n",
        "results = []\n",
        "out = []\n",
        "\n",
        "def apollo(tags_list, classes_list):\n",
        "  for i in tags:\n",
        "    for j in classes:\n",
        "      results.append(soup(i, j))\n",
        "\n",
        "def output(results_list):\n",
        "  for r in results_list:\n",
        "    if len(r) == 1:\n",
        "      for n in r:\n",
        "        out.append(n.text)\n",
        "    elif len(r) > 1:\n",
        "      for n in r:\n",
        "        out.append(n.text)\n",
        "  return out\n",
        "\n",
        "\n",
        "tags = [\"h1\", \"p\", \"span\", \"span\", \"p\", \"p\", \"div\", \"div\", \"span\"]\n",
        "classes = [\"field field-name-field-primary-title field-type-text field-label-hidden\", \n",
        "           \"field field-name-field-date-text field-type-text field-label-hidden\", \n",
        "           \"field field-name-field-artist-qualifier\",\n",
        "           \"field field-name-field-artist-name\",\n",
        "           \"field field-name-field-artist-origin\",\n",
        "           \"field field-name-art-object-medium field-type-ds field-label-hidden\",\n",
        "           \"field field-name-field-provenance-description\",\n",
        "           \"field field-name-field-provenance-date\",\n",
        "           \"field field-name-field-credit-line\"\n",
        "           ]\n",
        "\n",
        "apollo(tags, classes)\n",
        "\n",
        "output(results)\n",
        "\n",
        "# add our scraped data to dataframe\n",
        "index_list= [0,1,5,2,3,16,17,18,28]\n",
        "proc_data = [out[i] for i in index_list]\n",
        "df.loc[2]=proc_data\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Qv6GmbHZW9xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Note on the CMA's API"
      ],
      "metadata": {
        "id": "Se0ga9Lv_Lrs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we mentioned in the presentation, it is best practice to check for an API before you scrape.\n",
        "\n",
        "Rather incredibly, the CMA does have its own [API](https://openaccess-api.clevelandart.org/).  The institution was founded \"for the benefit of all the people forever\" and its online strategy deliberately upholds this mission.  Its web data, which includes descriptions, dates, provenances, and images, is unrestricted for both commercial and non-commercial use.\n",
        "\n",
        "To facilitate the use of the API, the museum even supplies tabulated code books and sample code.  The following example Python script uses the [Requests](https://requests.readthedocs.io/en/latest/user/quickstart/#make-a-request) library, which allows you to make HTTP/1.1 requests to a server and to read in the target webpage's HTML code, and the library JSON to parse it.\n",
        "\n",
        "In some ways, this API is a little unconventional.  It doesn't require you to register for a Key to access the content, and it still requires some general programming skills (some API services streamline the workflow for you).\n",
        "\n",
        "However, like the majority of APIs, this one delineates the data for you in an easy-to-read and import layout."
      ],
      "metadata": {
        "id": "2RwGbPaT1jTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code source: https://openaccess-api.clevelandart.org/\n",
        "# additional commentary by Moira O.\n",
        "\n",
        "import json\n",
        "\n",
        "\n",
        "def print_openaccess_results(keyword, skip=0, limit=100):\n",
        "    url = \"https://openaccess-api.clevelandart.org/api/artworks\"\n",
        "    params = {\n",
        "            'q': keyword,\n",
        "            'skip': skip,\n",
        "            'limit': limit,\n",
        "            'has_image': 1\n",
        "        }\n",
        "\n",
        "    r = requests.get(url, params=params)\n",
        "\n",
        "    data = r.json()\n",
        "\n",
        "    for artwork in data['data']:\n",
        "        tombstone = artwork['tombstone']\n",
        "        image = artwork['images']['web']['url']\n",
        "\n",
        "        print(f\"{tombstone}\\n{image}\\n---\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print_openaccess_results(\"monet\", 0, 10)\n"
      ],
      "metadata": {
        "id": "Kgjh5fqyiFFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test your knowledge\n",
        "\n",
        "Use your new webscraping skills to answer these three questions!"
      ],
      "metadata": {
        "id": "bIb3gYd5ds3X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1**: What year did Vincent Van Gogh paint \"The Large Plane Trees (Road Menders at Saint-RÃ©my)\"?"
      ],
      "metadata": {
        "id": "UvNWQSuseiaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use the URL below to answer this question:\n",
        "van_gogh = 'https://www.clevelandart.org/art/1947.209'\n",
        "\n",
        "# get request:\n",
        "\n",
        "\n",
        "# src usually the object label for the server's response:\n",
        "\n",
        "\n",
        "# Parse the response into an HTML tree:\n",
        "\n",
        "\n",
        "# Use the appropriate tag and class arguments to get the artwork's date:\n",
        "\n",
        "\n",
        "# print out the results:\n"
      ],
      "metadata": {
        "id": "AlXWapd0efwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2:** Who is the artist behind the CMA's photograph \"Camera Work: Steeplechase Day, Paris: After the Races\"?"
      ],
      "metadata": {
        "id": "-OZtTBKVy1nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use the URL below to answer this question:\n",
        "steeplechase = 'https://www.clevelandart.org/art/1995.199.42.k'\n",
        "\n",
        "# get request:\n",
        "\n",
        "\n",
        "# src usually the object label for the server's response:\n",
        "\n",
        "\n",
        "# Parse the response into an HTML tree:\n",
        "\n",
        "\n",
        "# Use the appropriate tag and class arguments to get the artist's name:\n",
        "\n",
        "\n",
        "# print out the results:\n"
      ],
      "metadata": {
        "id": "Q4ejVRfSzRpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3:** What medium(s) did Louise Bourgeois use to create \"Untitled c.1950\"?"
      ],
      "metadata": {
        "id": "Yv1gUt4JzSVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use the URL below to answer this question\n",
        "bourgeois = 'https://www.clevelandart.org/art/1998.112'\n",
        "\n",
        "# get request:\n",
        "\n",
        "\n",
        "# src usually the object label for the server's response:\n",
        "\n",
        "\n",
        "# Parse the response into an HTML tree:\n",
        "\n",
        "\n",
        "# Use the appropriate tag and class arguments to get the artist's name:\n",
        "\n",
        "\n",
        "# print out the results:\n"
      ],
      "metadata": {
        "id": "DFynciUBzVYp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}